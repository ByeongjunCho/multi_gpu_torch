{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nsmc' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/e9t/nsmc.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.arch = 'BERT_NSMC_singleGPU'\n",
    "        self.lr = 5e-5\n",
    "        self.batch_size = 32\n",
    "        self.gpu = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"kykim/bert-kor-base\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSMCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# load nsmc dataset\n",
    "nsmc_train = pd.read_csv('./nsmc/ratings_train.txt', sep='\\t', encoding='utf-8')\n",
    "nsmc_test = pd.read_csv('./nsmc/ratings_test.txt', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# slicing dataset\n",
    "nsmc_train = nsmc_train[:100000]\n",
    "nsmc_test = nsmc_test\n",
    "\n",
    "nsmc_train['document'] = nsmc_train['document'].apply(str)\n",
    "nsmc_test['document'] = nsmc_test['document'].apply(str)\n",
    "\n",
    "\n",
    "# encoding\n",
    "train_encodings = tokenizer(list(nsmc_train['document']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(nsmc_test['document']), truncation=True, padding=True)\n",
    "\n",
    "train_dataset = NSMCDataset(train_encodings, nsmc_train['label'])\n",
    "test_dataset = NSMCDataset(test_encodings, nsmc_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) * args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50016"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader) * args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and scheduler \n",
    "\n",
    "# layerNorm and bias do not trained\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# scheduler reference => https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup\n",
    "# using scheduler \n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, len(train_loader) * args.epochs // 8, len(train_loader)*args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "=================== test ==========================\n",
      "===================================================\n",
      "Epoch: 6 \n",
      "total_steps: 21874 \n",
      "loss: 0.388 \n",
      "test_acc : 90.102 \n",
      "\n",
      "Epoch 6 finished!. Save model\n",
      "====================================================\n",
      "=================== Training =======================\n",
      "====================================================\n",
      "Epoch: 7 \n",
      "total_steps: 21875 \n",
      "loss: 0.069 \n",
      "acc : 96.875 \n",
      "batch_time : 0.29984259605407715 \n",
      "\n",
      "Epoch: 7 \n",
      "total_steps: 22187 \n",
      "loss: 0.027 \n",
      "acc : 99.141 \n",
      "batch_time : 0.30092811584472656 \n",
      "\n",
      "Epoch: 7 \n",
      "total_steps: 22499 \n",
      "loss: 0.027 \n",
      "acc : 99.070 \n",
      "batch_time : 0.30440831184387207 \n",
      "\n",
      "Epoch: 7 \n",
      "total_steps: 22811 \n",
      "loss: 0.027 \n",
      "acc : 99.116 \n",
      "batch_time : 0.30628156661987305 \n",
      "\n",
      "Epoch: 7 \n",
      "total_steps: 23123 \n",
      "loss: 0.028 \n",
      "acc : 99.097 \n",
      "batch_time : 0.30324769020080566 \n",
      "\n",
      "Epoch: 7 \n",
      "total_steps: 23435 \n",
      "loss: 0.028 \n",
      "acc : 99.069 \n",
      "batch_time : 0.30539774894714355 \n",
      "\n",
      "Epoch: 7 \n",
      "total_steps: 23747 \n",
      "loss: 0.028 \n",
      "acc : 99.081 \n",
      "batch_time : 0.305239200592041 \n",
      "\n",
      "Epoch: 7 \n",
      "total_steps: 24059 \n",
      "loss: 0.028 \n",
      "acc : 99.096 \n",
      "batch_time : 0.3074514865875244 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9d7c1437dd36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "# Single GPU\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "writer = SummaryWriter(f'./runs/{args.arch}')\n",
    "print_train = len(train_loader) // 10\n",
    "total_epoch_start = time.time()\n",
    "\n",
    "# for load model\n",
    "best_acc = 0\n",
    "best_model_name = ''\n",
    "\n",
    "# for plot train and test loss\n",
    "train_iter_list = []\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_iter_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    model.cuda(args.gpu)\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('====================================================')\n",
    "    print('=================== Training =======================')\n",
    "    print('====================================================')\n",
    "\n",
    "    epoch_start = time.time()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        start = time.time()\n",
    "        inputs = {k: v.cuda(args.gpu) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs.loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += outputs.loss.item()\n",
    "        total += inputs['labels'].size(0)\n",
    "        correct += inputs['labels'].eq(outputs.logits.argmax(axis=1)).sum().item()\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        batch_time = time.time() - start\n",
    "        \n",
    "        train_iter_list.append(epoch * len(train_loader) + idx)\n",
    "        train_loss_list.append(outputs.loss.item())\n",
    "        train_acc_list.append(acc)\n",
    "        if idx % print_train == 0:\n",
    "            print(f'Epoch: {epoch} \\n'\n",
    "                  f'total_steps: {epoch * len(train_loader) + idx} \\n'\n",
    "                  f'loss: {train_loss / (idx+1):.3f} \\n'\n",
    "                  f'acc : {acc:.3f} \\n'\n",
    "                  f'batch_time : {batch_time} \\n'\n",
    "                  )\n",
    "        writer.add_scalar('Loss/train',\n",
    "                           outputs.loss.item(),\n",
    "                           epoch * len(train_loader) + idx)\n",
    "        writer.add_scalars('Loss', \n",
    "                           {'Train_Loss': outputs.loss.item()}, \n",
    "                          epoch * len(train_loader) + idx)\n",
    "        writer.add_scalar('Accuracy/train',\n",
    "                          acc,\n",
    "                          epoch * len(train_loader) + idx)\n",
    "        \n",
    "    \n",
    "    # test\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    elapse_time = time.time() - epoch_start\n",
    "    elapse_time = datetime.timedelta(seconds=elapse_time)\n",
    "    print(f\"Epoch training: {elapse_time}\")\n",
    "    clear_output(wait=True)\n",
    "    # test\n",
    "    print('===================================================')\n",
    "    print('=================== test ==========================')\n",
    "    print('===================================================')\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            start = time.time()\n",
    "            inputs = {k: v.cuda(args.gpu) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            total += inputs['labels'].size(0)\n",
    "            correct += inputs['labels'].eq(outputs.logits.argmax(axis=1)).sum().item()\n",
    "            test_loss += outputs.loss.item()\n",
    "            acc = 100 * correct / total\n",
    "            \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # write test result\n",
    "    test_iter_list.append(epoch * len(train_loader) + idx)\n",
    "    test_loss_list.append(test_loss)\n",
    "    test_acc_list.append(acc)\n",
    "    print(f'Epoch: {epoch} \\n'\n",
    "          f'total_steps: {epoch * len(train_loader) + idx} \\n'\n",
    "          f'loss: {test_loss:.3f} \\n'\n",
    "          f'test_acc : {acc:.3f} \\n'\n",
    "          )\n",
    "    writer.add_scalar('Loss/test',\n",
    "                       test_loss,\n",
    "                       epoch * len(train_loader) + idx\n",
    "                       )\n",
    "    writer.add_scalars('Loss', \n",
    "                       {'Test_Loss': test_loss},\n",
    "                       epoch * len(train_loader) + idx\n",
    "                      )\n",
    "    writer.add_scalar('Accuracy/test', \n",
    "                     acc, \n",
    "                     epoch * len(train_loader) + idx)\n",
    "    \n",
    "    print(f'Epoch {epoch} finished!. Save model')\n",
    "    os.makedirs(f'./{args.arch}', exist_ok=True)\n",
    "    # model parameter save per epoch. \n",
    "    torch.save(model.state_dict(), f'./{args.arch}/{epoch}_{test_loss:.3f}_{acc:.3f}.pt')\n",
    "    if best_acc < acc:\n",
    "        best_acc = acc\n",
    "        best_model_name = f'./{args.arch}/{epoch}_{test_loss:.3f}_{acc:.3f}.pt'\n",
    "        \n",
    "print(f'Total training time: {time.time() - total_epoch_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "=================== test ==========================\n",
      "===================================================\n",
      "0 - loss: 0.7621586322784424\n",
      "100 - loss: 0.3927501440048218\n",
      "200 - loss: 0.6057535409927368\n",
      "300 - loss: 0.5997530221939087\n",
      "400 - loss: 0.28308576345443726\n",
      "500 - loss: 0.300104022026062\n",
      "600 - loss: 0.1321668177843094\n",
      "700 - loss: 0.22531689703464508\n",
      "800 - loss: 0.24568776786327362\n",
      "900 - loss: 0.32960253953933716\n",
      "1000 - loss: 0.805182933807373\n",
      "1100 - loss: 0.24527928233146667\n",
      "1200 - loss: 0.24607160687446594\n",
      "1300 - loss: 0.14246365427970886\n",
      "1400 - loss: 0.6441746354103088\n",
      "1500 - loss: 0.34215572476387024\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "clear_output(wait=True)\n",
    "\n",
    "test_loss = 0\n",
    "total = 0\n",
    "correct = 0\n",
    "# test\n",
    "print('===================================================')\n",
    "print('=================== test ==========================')\n",
    "print('===================================================')\n",
    "with torch.no_grad():\n",
    "    for idx, inputs in enumerate(test_loader):\n",
    "        start = time.time()\n",
    "        inputs = {k: v.cuda(args.gpu) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        total += inputs['labels'].size(0)\n",
    "        correct += inputs['labels'].eq(outputs.logits.argmax(axis=1)).sum().item()\n",
    "        test_loss += outputs.loss.item()\n",
    "        acc = 100 * correct / total\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(f'{idx} - loss: {outputs.loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207452"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222528"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3943, -4.1652],\n",
       "        [-3.5154,  4.1229],\n",
       "        [-2.9718,  3.2908],\n",
       "        [-1.2286,  1.8589],\n",
       "        [ 3.4289, -4.2430],\n",
       "        [ 3.4299, -4.2364],\n",
       "        [ 3.4250, -4.2448],\n",
       "        [-3.4777,  4.1581],\n",
       "        [-3.4675,  4.1722],\n",
       "        [-3.4856,  4.1616],\n",
       "        [-2.1384,  2.6234],\n",
       "        [-3.5232,  3.9496],\n",
       "        [ 3.4252, -4.2436],\n",
       "        [ 3.3885, -4.1542],\n",
       "        [ 3.4289, -4.2480],\n",
       "        [ 1.6961, -1.2069]], device='cuda:7')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48114596343403676"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751.0741811843473"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4805337051723271"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0071, device='cuda:7')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9569596630521356"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.0496615042196"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3125, 1563)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter_list = []\n",
    "# train_loss_list = []\n",
    "# train_acc_list = []\n",
    "# test_iter_list = []\n",
    "# test_loss_list = []\n",
    "# test_acc_list = []\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and test loss\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(train_iter_list, train_loss_list, color='r', label='Train_Loss')\n",
    "ax.scatter(test_iter_list, test_loss_list, color='b', label='Test_Loss')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_state_dict(torch.load(best_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "model.cuda(args.gpu)\n",
    "# test\n",
    "print('===================================================')\n",
    "print('=================== test ==========================')\n",
    "print('===================================================')\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        start = time.time()\n",
    "        inputs = {k: v.cuda(args.gpu) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        total += inputs['labels'].size(0)\n",
    "        correct += inputs['labels'].eq(outputs.logits.argmax(axis=1)).sum().item()\n",
    "        test_loss += outputs.loss.item()\n",
    "        acc = 100 * correct / total\n",
    "\n",
    "test_loss = test_loss / len(test_loader)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
